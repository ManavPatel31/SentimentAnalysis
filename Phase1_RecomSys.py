# -*- coding: utf-8 -*-
"""ProjectLatest-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Px05M3jm3ne-HenUMJI34n7aQ9EstZYC
"""

import pandas as pd
import matplotlib.pyplot as plt

data_group2 = pd.read_json('/content/All_Beauty_5.json',lines=True)
print(data_group2.head(5))

# Number of Reviews per Product
review_counts = data_group2.groupby(['asin'])['reviewerID'].count()
print(review_counts)

# Number of Reviews across Products
overall_counts = data_group2.groupby(['asin','overall'],sort=True)['reviewerID'].count()
print(overall_counts)

# Number of Reviews per User
user_counts = data_group2.groupby(['asin','reviewerID'])['reviewerName'].count()
print(user_counts)

print('Number of Distinct Products: ',data_group2['asin'].nunique())
print('Number of Distinct Users: ', data_group2['reviewerID'].nunique())
print('Number of Distinct User Names: ', data_group2['reviewerName'].nunique())

type(data_group2['reviewText'])

reviews_len = data_group2['reviewText'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)
summary_len = data_group2['summary'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)

# Getiing the average of that
average_reviews_len = reviews_len.mean()
average_summary_len = summary_len.mean()

print(average_reviews_len)
print(average_summary_len)

# Randomly choosing a sample of data for text preprocessing
sample_df = data_group2.sample(n=830)

# Labelling the rating based on the overall

def label_data(rating):
    if(rating in [4,5]):
        return 'Positive'
    elif(rating == 3):
        return 'Neutral'
    else:
        return 'Negative'

sample_df['Rating of Product'] = sample_df['overall'].apply(label_data)

# we do not need the columns below because they do not in any way help in the analysis.
print(sample_df.columns.values)
sample_df = sample_df.drop(columns=['unixReviewTime','overall','reviewerName','reviewTime'])
print(sample_df.columns.values)

sample_df['review_len'] = sample_df['reviewText'].apply(lambda x: len(x.split())if isinstance(x, str) else 0)
sample_df['Rating of Product'].value_counts()

positive_words = pd.read_csv('/content/positive-words.txt',sep="\t",encoding='latin1',header=None)
negative_words = pd.read_csv('/content/negative-words.txt',sep="\t",encoding='latin1',header=None)

positive_words.columns = ["words"]
negative_words.columns = ["words"]

pos_set = set(list(positive_words["words"]))
neg_set = set(list(negative_words["words"]))
print (len(pos_set))
print (len(neg_set))

sample_df['review_processed_docs'] = [doc.lower().replace(".", "") if isinstance(doc, str) else "" for doc in sample_df['reviewText']]
print(sample_df)

print(sample_df['review_processed_docs'])

"""# New section"""

sample_df['review_processed_docs_1']=sample_df['review_processed_docs']

sample_df = sample_df.drop(columns=['image'],axis =1)

print(sample_df.columns.values)

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords

stopwords = set(stopwords.words("english"))

print(stopwords)

type(stopwords)

stopwords_negative = {"won't",'wouldn','isn',"wouldn't",'no','didn','wasn','nor','not',"wasn't","couldn't","mightn't","mustn't","doesn't","weren't","didn't","shouldn't",'doesn','against','hadn',"haven't",'couldn','mustn',"aren't","hasn't","shan't","don't", "hadn't", "isn't","needn't"}

stopwords2 = stopwords - stopwords_negative
#stopwords.discard('never')

print(stopwords2)

sample_df['review_processed_docs']

from nltk.tokenize import word_tokenize

sample_df['review_processed_docs'] = sample_df['review_processed_docs'].apply(lambda x: word_tokenize(str(x)))

print(sample_df['review_processed_docs'])

sample_df['review_processed_docs'] = sample_df['review_processed_docs'].apply(lambda doc: [word for word in doc if word not in stopwords2])

print(sample_df['review_processed_docs'])

## TF-IDF Text Representation

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()
bow_rep_tfidf = tfidf.fit_transform(sample_df['review_processed_docs'].apply(lambda x: ' '.join(x)))

#IDF for all words in the vocabulary
print("IDF for all words in the vocabulary",tfidf.idf_)
print("-"*10)

#All words in the vocabulary.
print("All words in the vocabulary",tfidf.vocabulary_)
print("-"*10)

#TFIDF representation for all documents in our corpus 
print("TFIDF representation for all documents in our corpus\n",bow_rep_tfidf.toarray()) 
print("-"*10)

## Word2Vec Text Representation

from gensim.models import Word2Vec

corpus = sample_df['review_processed_docs']

model_cbow = Word2Vec(corpus, min_count=1,sg=0)
print(model_cbow)

words = list(model_cbow.wv.index2entity)
print(words)

print( model_cbow.wv['body'].reshape((1, 100)))
print("Similarity between soap and wash:",model_cbow.wv.similarity('soap', 'wash'))

!pip install vaderSentiment

##Sentiment Analysis created using VADER

sample_df['review_processed_docs'] = sample_df['review_processed_docs'].apply(lambda x: ' '.join(x))
print(sample_df['review_processed_docs'])

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
sa = SentimentIntensityAnalyzer()

for statements in sample_df['review_processed_docs_1']:
    print(f'For statement : "{statements}"')
    print("-------------------------------------------------")
    polar = sa.polarity_scores(statements)
    positive = polar["pos"]
    neutral = polar["neu"]
    negative = polar["neg"]
    compound = polar['compound']
    
    print(f'The % of positive sentiment in :-- "{statements}" is : {round(positive*100,2)} %')
    print("-------------------------------------------------")
    print(f'The % of neutral sentiment in :-- "{statements}" is : {round(neutral*100,2)} %')
    print("-------------------------------------------------")
    print(f'The % of Compound sentiment in :-- "{statements}" is : {round(compound*100,2)} %')
    print("-------------------------------------------------")
    print(f'The % of negative sentiment in :-- "{statements}" is : {round(negative*100,2)} %')
    print("-------------------------------------------------")
    print("-"*30)

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
sa = SentimentIntensityAnalyzer()

for statements in sample_df['review_processed_docs']:
    print(f'For statement : "{statements}"')
    print("-------------------------------------------------")
    polar = sa.polarity_scores(statements)
    positive = polar["pos"]
    neutral = polar["neu"]
    negative = polar["neg"]
    compound = polar['compound']
    
    print(f'The % of positive sentiment in :-- "{statements}" is : {round(positive*100,2)} %')
    print("-------------------------------------------------")
    print(f'The % of neutral sentiment in :-- "{statements}" is : {round(neutral*100,2)} %')
    print("-------------------------------------------------")
    print(f'The % of Compound sentiment in :-- "{statements}" is : {round(compound*100,2)} %')
    print("-------------------------------------------------")
    print(f'The % of negative sentiment in :-- "{statements}" is : {round(negative*100,2)} %')
    print("-------------------------------------------------")
    print("-"*30)



## Sentiment Analysis using TextBlob

import re

sample_df['review_processed_docs'] = sample_df['review_processed_docs'].apply(lambda x: re.sub(r'[^\w\s\']', '', x))

pip install textblob

from textblob import TextBlob
sample_df['Sentiment_polarity'] = sample_df['review_processed_docs'].apply(lambda x: TextBlob(x).sentiment.polarity)
print(sample_df['Sentiment_polarity'])

text = str(sample_df['review_processed_docs'])

print(text)

from textblob import Word

text = TextBlob(text)

nltk.download('averaged_perceptron_tagger')

print(text.tags)

"""CC coordinating conjunction
CD cardinal digit
DT determiner
EX existential there (like: “there is” … think of it like “there exists”)
FW foreign word
IN preposition/subordinating conjunction
JJ adjective ‘big’
JJR adjective, comparative ‘bigger’
JJS adjective, superlative ‘biggest’
LS list marker 1)
MD modal could, will
NN noun, singular ‘desk’
NNS noun plural ‘desks’
NNP proper noun, singular ‘Harrison’
NNPS proper noun, plural ‘Americans’
PDT predeterminer ‘all the kids’
POS possessive ending parent‘s
PRP personal pronoun I, he, she
PRP$ possessive pronoun my, his, hers
RB adverb very, silently,
RBR adverb, comparative better
RBS adverb, superlative best
RP particle give up
TO to go ‘to‘ the store.
UH interjection errrrrrrrm
VB verb, base form take
VBD verb, past tense took
VBG verb, gerund/present participle taking
VBN verb, past participle taken
VBP verb, sing. present, non-3d take
VBZ verb, 3rd person sing. present takes
WDT wh-determiner which
WP wh-pronoun who, what
WP$ possessive wh-pronoun whose
WRB wh-adverb where, when
"""

sentence = text.sentences
print(sentence)

sentence = str(sentence)
print(type(sentence))

textwords = text.words
print(textwords)

for word in textwords:
  frequency = text.word_counts[word]
  print(word,frequency)

